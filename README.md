# 🌐 Translator

A powerful and modular Neural Machine Translation (NMT) system built completely from scratch using the **Transformer architecture**. This project demonstrates how deep learning and attention mechanisms can be applied to translate text between human languages.
It is the part of MINOR PROJECT , Computer Engineering , 3rd year, IOE Pulchowk.

---

## 🚀 Features

- ✅ Custom-built Transformer model (no pre-trained model used)
- ✅ Encoder–Decoder attention mechanism
- ✅ Multi-head Self-Attention & Positional Encoding
- ✅ Trainable on any parallel corpus
- ✅ Translate sentences on streamlit app linked
- ✅ Well-structured and extensible codebase
- ✅ Preprocessed HuggingFace datasets

---

## 🧠 Architecture Overview

This project is inspired by the groundbreaking paper:  
📄 *"Attention Is All You Need"* – Vaswani et al., 2017

It implements the following components:

- **Encoder**: Processes the source sentence
- **Decoder**: Generates the target sentence
- **Multi-head Scaled Dot-Product Attention**
- **Feed-Forward Networks**
- **Layer Normalization**
- **Positional Encoding**

The entire architecture is built using only core Python libraries and PyTorch.

---

## 📚 References
- Vaswani et al. (2017), "Attention Is All You Need"
- HuggingFace Datasets

---

## 🧑‍💻 Authors and 🤝 Contributors

- **Aditya Rijal** – [@Aadi-Rijal](https://github.com/Aadi-Rijal)
- **Aayush Regmi** – [@AayusR](https://github.com/AayusR)
- **Awiskar Pokhrel** – [@awiskar3](https://github.com/awiskar3)
