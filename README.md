# ğŸŒ Translator

A powerful and modular Neural Machine Translation (NMT) system built completely from scratch using the **Transformer architecture**. This project demonstrates how deep learning and attention mechanisms can be applied to translate text between human languages.
It is the part of MINOR PROJECT , Computer Engineering , 3rd year, IOE Pulchowk.

---

## ğŸš€ Features

- âœ… Custom-built Transformer model (no pre-trained model used)
- âœ… Encoderâ€“Decoder attention mechanism
- âœ… Multi-head Self-Attention & Positional Encoding
- âœ… Trainable on any parallel corpus
- âœ… Translate sentences on streamlit app linked
- âœ… Well-structured and extensible codebase
- âœ… Preprocessed HuggingFace datasets

---

## ğŸ§  Architecture Overview

This project is inspired by the groundbreaking paper:  
ğŸ“„ *"Attention Is All You Need"* â€“ Vaswani et al., 2017

It implements the following components:

- **Encoder**: Processes the source sentence
- **Decoder**: Generates the target sentence
- **Multi-head Scaled Dot-Product Attention**
- **Feed-Forward Networks**
- **Layer Normalization**
- **Positional Encoding**

The entire architecture is built using only core Python libraries and PyTorch.

---

## ğŸ“š References
- Vaswani et al. (2017), "Attention Is All You Need"
- HuggingFace Datasets

---

## ğŸ§‘â€ğŸ’» Authors and ğŸ¤ Contributors

- **Aditya Rijal** â€“ [@Aadi-Rijal](https://github.com/Aadi-Rijal)
- **Aayush Regmi** â€“ [@AayusR](https://github.com/AayusR)
- **Awiskar Pokhrel** â€“ [@awiskar3](https://github.com/awiskar3)
